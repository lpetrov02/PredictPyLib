# -*- coding: utf-8 -*-
# """
# guess_lib_final.ipynb
# 
# Automatically generated by Colaboratory.
# 
# Original file is located at
#     https://colab.research.google.com/drive/1bZV8JBwoKROHaE-xJPbb050jPtX4cU1F
# """


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn

from sklearn.model_selection import train_test_split

import time

import nltk
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import wordpunct_tokenize
from nltk.stem import PorterStemmer

from dataclasses import dataclass

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import log_loss, accuracy_score, top_k_accuracy_score

train_data = pd.read_csv("./data/train.csv")
test_data = pd.read_csv("./data/test.csv")

print(f"Train data shape: {train_data.shape}")
print(f"Test data shape: {test_data.shape}")

print(train_data[pd.isna(train_data["lib"])])

train_data = train_data.dropna()

# libs, counts = np.unique(train_data["lib"].tolist(), return_counts=True)
# plt.figure(figsize=(16, 10))
# plt.bar(libs, counts)
# Тут смотрим на гистограмму

# 0. Предобработка данных

data_x, data_y = np.array(train_data.loc[:, "title"]), np.array(train_data.loc[:, "lib"])
data_test_x = np.array(test_data.loc[:, "title"])

labels = np.unique(data_y)
label_to_idx = {label: i for i, label in enumerate(labels)}
n_classes = len(labels)
print("Labels:")
print(labels, n_classes, '\n')

def preprocess(titles_x, labels_y):
    X, y = [], []
    stemmer = PorterStemmer()
    stopWords = set(stopwords.words('english')) | set(["'", "\"", ",", ".", "?", "!"])
    for title, label in zip(titles_x, labels_y):
        tokens = wordpunct_tokenize(title.lower())
        tokens = list(map(stemmer.stem, tokens))
        proc_title = " ".join([token for token in tokens if token not in stopWords]).replace("&", "and")
        X.append(proc_title)
        y.append(label_to_idx[label])
    return np.array(X), np.array(y)

data_x, data_y = preprocess(data_x, data_y)

print(list(zip(data_x[:5], data_y[:5])))

# Для данной задачи пунктуация не важна, ровно как и эмоциональная окраска (которая может выражаться через "?" и "!")

X_train, X_val, y_train, y_val = train_test_split(data_x, data_y)
print("X_train shape: ", X_train.shape, "y_train shape: ", y_train.shape)

# 1. Бейзлайн
# Пусть это будет простая логистическая регрессия с *tf-idf* фичами

pipeline = Pipeline(
    [('vectorize', CountVectorizer()), ('transform', TfidfTransformer()), ('logregression', LogisticRegression(max_iter=1000))]
)

param_grid = {
    "logregression__C": [0.01, 0.025, 0.05, 0.1, 1, 10],
}

search_cv = GridSearchCV(pipeline, param_grid, verbose=3)
search_cv.fit(X_train, y_train)

best_pipeline = search_cv.best_estimator_
y_pred = best_pipeline.predict(X_val)
y_pred_probas = best_pipeline.predict_proba(X_val)
print("First predictions: ", y_pred[:5])

print("Best hyperparams: ", search_cv.best_params_)

print(f"Log loss: {log_loss(y_val, y_pred_probas)}")
print(f"Top-1 accuracy: {accuracy_score(y_val, y_pred)}")
print(f"Top-3 accuracy: {top_k_accuracy_score(y_val, y_pred_probas, k=3)}")

# Время предсказывать для теста

def preprocess_features(titles_x):
    X = []
    stemmer = PorterStemmer()
    stopWords = set(stopwords.words('english')) | set(["'", "\"", ",", ".", "?", "!"])
    for title in titles_x:
        tokens = wordpunct_tokenize(title.lower())
        tokens = list(map(stemmer.stem, tokens))
        proc_title = " ".join([token for token in tokens if token not in stopWords]).replace("&", "and")
        X.append(proc_title)
    return np.array(X)

data_test_x = preprocess_features(data_test_x)

test_predictions = best_pipeline.predict(data_test_x)

test_data["lib"] = np.array([labels[i] for i in test_predictions])

test_data.to_csv("./data/leonid_petrov.csv")

# Были попытки взять предобученную Roberta-base с huggingface в качестве фича-экстрактора, но они,
# видимо, из-за недостаточно хорошей архитектуры моей модели, дали худшее качество
